{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation**\n",
        "* **Introduces Embeddings-Based Memory**\n",
        "  Teaches how to convert text into vector representations using OpenAI's embedding models.\n",
        "\n",
        "* **Uses ChromaDB for Vector Storage**\n",
        "  Demonstrates how an AI agent can store and search through vectorized documents with persistent memory.\n",
        "\n",
        "* **Loads External PDF Knowledge**\n",
        "  Prepares students to bring in real-world knowledge from documents (e.g., PDF reports, manuals, articles).\n",
        "\n",
        "* **Performs Similarity Search**\n",
        "Uses ChromaDB to find relevant context based on semantic meaning, not just keyword matching.\n",
        "\n",
        "* **Injects Retrieved Context into LLM Prompt**\n",
        "  Shows how to ground LLM responses by combining retrieved knowledge with user questions.\n",
        "\n",
        "* **Implements Retrieval-Augmented Generation Lite**\n",
        "Lays the foundation for full RAG systems by walking through the basic building blocks:\n",
        "Document → Embed → Store → Retrieve → Inject → Respond\n",
        "\n",
        "* **Console-Based and Interactive**\n",
        "Keeps the experience user-driven, helping learners understand how input affects output at each stage.\n",
        "\n",
        "* **Prepares for Full RAG Pipelines**\n",
        "Sets up future upgrades like:\n",
        "LangChain Retriever & RAGChain\n",
        "Prompt templates\n",
        "Metadata filtering\n",
        "Chunking strategies"
      ],
      "metadata": {
        "id": "pvgx8W-1EQNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai --quiet\n",
        "!pip install -U chromadb --quiet\n",
        "!pip install -U sentence-transformers --quiet\n",
        "!pip install -U pymupdf --quiet"
      ],
      "metadata": {
        "id": "vyCZHG2y8pRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Mount Google Drive to access API key ===\n",
        "from google.colab import drive\n",
        "drive_path = '/content/drive'\n",
        "drive.mount(drive_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLsuq3Zi8n7P",
        "outputId": "9cb22608-38bc-4832-e6c6-b1de18ef5164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "secret_file_path = \"/content/drive/My Drive/Secret_Keys/OpenAI_Secret_Key.json\"\n",
        "\n",
        "with open(secret_file_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "secret_Key = data[\"openai_api_key\"]"
      ],
      "metadata": {
        "id": "CbhnqfSW8pGU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f9d81c4f-b238-40ac-c946-c082adbd44e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from uuid import uuid4\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "bigPMpTVBl3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure OpenAI key (use your own)\n",
        "os.environ[\"OPENAI_API_KEY\"] = secret_Key\n",
        "client = OpenAI()\n",
        "\n",
        "# ChromaDB setup\n",
        "PERSIST_DIR = \"/content/chroma_rag_pdf\"\n",
        "chroma_client = chromadb.Client(Settings(persist_directory=PERSIST_DIR, anonymized_telemetry=False))\n",
        "collection = chroma_client.get_or_create_collection(name=\"rag_pdf_collection\")\n",
        "\n",
        "# Load SentenceTransformer model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "dl0COHjJBotG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read PDF text from Google Drive path\n",
        "def load_pdf_text(drive_path):\n",
        "    doc = fitz.open(drive_path)\n",
        "    all_text = \"\"\n",
        "    for page in doc:\n",
        "        all_text += page.get_text()\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "A2AQesngBvPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking logic (simple split by paragraph)\n",
        "def chunk_text(text, max_len=300):\n",
        "    chunks = []\n",
        "    current = \"\"\n",
        "    for para in text.split(\"\\n\"):\n",
        "        if len(current) + len(para) < max_len:\n",
        "            current += para + \"\\n\"\n",
        "        else:\n",
        "            chunks.append(current.strip())\n",
        "            current = para + \"\\n\"\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "mD_42MZCBy8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest PDF into ChromaDB\n",
        "def ingest_pdf(drive_pdf_path):\n",
        "    text = load_pdf_text(drive_pdf_path)\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = embedder.encode(chunks).tolist()\n",
        "    ids = [str(uuid4()) for _ in chunks]\n",
        "    collection.add(ids=ids, documents=chunks, embeddings=embeddings)\n",
        "    print(f\"Ingested {len(chunks)} chunks from {drive_pdf_path}\")"
      ],
      "metadata": {
        "id": "jTsM0AXnB26A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query ChromaDB + Inject into LLM prompt\n",
        "def ask_question_with_context(question, top_k=5):\n",
        "    query_embedding = embedder.encode([question]).tolist()[0]\n",
        "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
        "    relevant_chunks = results[\"documents\"][0]\n",
        "\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "    prompt = f\"Use the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    print(\"\\nLLM Answer:\\n\" + response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "zQIbSH7nB7Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejTERxZp8kR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b88511d-b835-4aba-b524-e624d55a05bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG PDF Agent (type 'ingest <google_drive_path>', 'ask <question>', or 'exit')\n",
            "You > exit\n"
          ]
        }
      ],
      "source": [
        "#  Console-based interaction\n",
        "def run_rag_agent():\n",
        "    print(\"RAG PDF Agent (type 'ingest <google_drive_path>', 'ask <question>', or 'exit')\")\n",
        "    while True:\n",
        "        user_input = input(\"You > \").strip()\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "        elif user_input.startswith(\"ingest\"):\n",
        "            parts = user_input.split(\" \", 1)\n",
        "            if len(parts) == 2:\n",
        "                ingest_pdf(parts[1])\n",
        "            else:\n",
        "                print(\"Usage: ingest <google_drive_path>\")\n",
        "        elif user_input.startswith(\"ask\"):\n",
        "            parts = user_input.split(\" \", 1)\n",
        "            if len(parts) == 2:\n",
        "                ask_question_with_context(parts[1])\n",
        "            else:\n",
        "                print(\"Usage: ask <your question>\")\n",
        "        else:\n",
        "            print(\"Unknown command. Use 'ingest <path>' or 'ask <question>'\")\n",
        "\n",
        "# Start the agent\n",
        "run_rag_agent()"
      ]
    }
  ]
}